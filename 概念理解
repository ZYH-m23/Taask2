神经网络是由多层神经元组成的深度学习模型，核心分3部分：
 
1. 输入层：接收原始数据（比如图片的像素值、文字的编码），是数据进模型的入口；
2. 隐藏层：核心计算层（可设1层或多层），靠权重做线性/非线性运算，提取数据里的关键特征（比如图片的轮廓、纹理）；
3. 输出层：给出最终结果（比如“猫”“狗”的分类、预测的数字）。
它的核心能力是通过训练自动学习特征与结果的映射关系，不用人工定义规则，能搞定图片识别、语音转文字、大模型对话等复杂任务
因为神经网络只能处理“向量（一堆数字）”，不能直接理解“编号

映射：
映射的本质是计算，之前学的张量、张量操作，就是这个映射的载体——隐藏层里的每一次张量运算，都是在调整映射规则，训练的过程就是把“不准的映射”调成“精准的映射”。

V=vocab_size
𝑑=𝑑_𝑚𝑜𝑑e𝑙
意思是：
这是一个 V 行、d 列的矩阵
行：编号
列：每个编号用几个数字表示
forward ：当数据流到我这个零件时，我要对它做哪些操作
self.embedding[x]
用 x 里的数字，去 self.embedding 这张表里“按行号取数据

非线性函数（激活函数）：神经网络能表示复杂的函数关系，比如图像、语言的模式。


SwiGLU
作用：是一种 激活函数和门控机制的结合，常用于 Transformer 或 MLP 层。
公式概念：
通常把输入向量拆成两部分 
x1和 x2：
SwiGLU
(𝑥1,𝑥2)=𝑥1⋅SiLU(𝑥2)
其中 
SiLU(𝑥=𝑥⋅𝜎(𝑥)
作用：𝑥1 是“信息通道”，𝑥2是“门控通道”
通过 Sigmoid 控制哪些信息可以通过，实现更灵活的特征选择

AbsTopK
概念：AbsTopK 是一种 稀疏激活函数，论文中用来提高可解释性。
作用：对输入向量取绝对值，然后只保留前 𝑘个最大的元素，其余元素置零，这样做可以让网络只关注最重要的特征，减少噪声

可解释性（Interpretability）
概念：指模型的内部计算和输出结果可以被人理解或解释，而不是黑箱。
在这里的作用：使用 AbsTopK 激活函数，可以让我们清楚地看到 哪些特征被模型认为最重要，从而提高模型的可解释性。

前馈网络（Feed-Forward Network, FFN）
位置：Transformer 每个层都有两大模块：
自注意力层（Self-Attention）
前馈网络（FFN）
作用：前馈网络负责 对每个位置的向量独立地进行非线性变换，帮助模型捕捉复杂的特征和模式。
结构：通常是两个线性层 + 一个激活函数，例如：
FFN(𝑥)=𝑊2⋅𝜎(𝑊1𝑥+𝑏1)+𝑏2
W1,W2是权重矩阵，σ 是激活函数（如 ReLU、SiLU、SwiGLU）
直观理解：FFN 就像给每个词的向量做“加工”，提炼出更复杂、更抽象的特征

FFN：在 Transformer 中，帮助模型把每个位置的向量做非线性加工，提取复杂特征
SwiGLU：比普通激活函数更智能，结合了门控机制和非线性变换，让模型能选择性放大重要信息
FFN = 每个词的特征“加工处理器”，帮提炼信息
SwiGLU = 带智能开关的加工方式，只保留最重要的部分

GLU 是一种 激活函数 + 门控机制 的组合，用在神经网络里。
核心思想：让网络学会“选择性地放行信息”，而不是所有信息都通过。

SiLU激活函数
Sigmoid(x) 的值在 0~1 之间，所以 SiLU(x) 的作用就是 有选择地缩放 x：
当 x 很小或很负 → Sigmoid(x) ≈ 0 → 输出接近 0
当 x 很大 → Sigmoid(x) ≈ 1 → 输出 ≈ x
中间值 → 平滑过渡

