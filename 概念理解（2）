SwiGLU 的核心思想
（1）内容分支：提取信息
给每个词的向量做一次处理，得到“内容向量”。
内容向量是向量的重要信息集合，但还没有被筛选。
通过 SiLU 激活函数，内容向量被平滑处理，使信息更加丰富。
（2）门控分支：生成开关
同样输入原始向量，生成一个“门控信号向量”。
这个向量决定哪些内容是重要的，哪些可以忽略。
门控信号的数值越大 → 允许更多信息通过；越小 → 阻止信息通过
（3）门控相乘：筛选信息
把内容向量和门控信号向量逐个元素相乘。
这样，只有重要信息会保留下来，其余信息被压缩或抑制
（4）输出映射：整理结果
把筛选后的信息再映射回原来的向量维度。
输出向量可以直接送到下一层 Transformer，继续处理

隐藏维度，就是你在 中间层映射到的空间大小。比如 hidden_dim = 128，表示经过这一层的线性变换后，每个词会变成 128 维的向量。
为什么要用隐藏维度？
提取特征：
原始 d_model 的信息可能比较紧凑，隐藏维度可以让网络有更多“空间”去组合和学习特征。
增加模型表达能力：
如果隐藏维度太小，模型可能学不到复杂的模式。
太大，可能参数太多，容易过拟合。
方便门控：
在 GLU 或门控前馈中，我们把内容信息和门控信号映射到同一个隐藏维度后再做逐元素相乘，这样更灵活

标准密集模型：就是普通的神经网络，每个神经元都和很多输入特征连接，没有特别的限制。

神经元多义（Polysemantic）：一个神经元并不是只表示一个清晰的概念，而是可能在不同情况下被多个完全不同的概念激活。
理解：有限的神经元空间 → 神经元承担多重概念 → 多义激活。

激活稀疏性（Activation Sparsity）：
指在神经网络中，大部分神经元保持不激活（输出接近 0），只有少数神经元被“点亮”。

Top-K 神经元：
在每一层或每一时刻，只让输出最大的 K 个神经元激活，其余神经元全部置为 0。

概念解耦（Decoupling Concepts）：
让每个神经元或神经元组合只专注表示一个明确的概念，而不是混合多个概念

语句理解
激活稀疏性可以迫使模型解耦这些概念。
解释：
如果模型允许所有神经元同时激活，它可能把多个概念混合在同一个神经元里 → 多义神经元。
如果模型被限制每次只能少数神经元激活，就没有空间让一个神经元“多用”了。
于是模型必须把不同概念分配给不同的神经元或神经元组合 → 概念被解耦，清晰分开。


常见格式

nn.Module 是 PyTorch 所有神经网络模块（层）的父类。
它定义了很多功能，比如：
管理参数（nn.Parameter）
保存子模块（self.add_module()）
自动把模块移动到 GPU（model.to(device)）
保存和加载模型（state_dict() / load_state_dict()）

super().__init__()
注册模块参数：让所有 nn.Parameter 和子模块能被 PyTorch 追踪。
初始化子模块列表：你之后可以在类里定义子模块，例如 self.w1 = nn.Linear(...)，父类会自动管理它们。
启用 forward 调用机制：父类知道你的 forward 方法是什么，保证 model(x) 可以自动调用 forward。
允许模块组合：父类会管理模块之间的层级关系，使得保存/加载模型、转 GPU 等操作正常工作。
super().__init__() 的作用是让你的自定义神经网络模块继承 PyTorch 的管理能力，保证参数、子模块、GPU 转移、保存加载等功能正常工作

forward
在 PyTorch 里，每个自定义模块（继承 nn.Module）都会有一个叫 forward 的方法。
它定义了模块如何处理输入得到输出。
换句话说，forward 就是 前向传播的逻辑：输入 → 处理 → 输出
在神经网络里，层与层之间的张量维度必须一致才能相连

在 Python 中，如果一个对象的类继承自 nn.Module，并且定义了 forward 方法，那么你可以像调用函数一样直接用括号调用对象：
